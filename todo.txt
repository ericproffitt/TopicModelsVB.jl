1. smooth out fLDA and fCTM
2. figure out way to preserve ordering of documents so showdocs() is comprehensible
3. use feather or paraquet or some fead read-in binary file format for default corpora in order to speed up their loading times.
4. deal with counts situation
5. why is nu variable used in Newton's method for alpha in LDA but not vsq in CTM?
6. when different vocab keys map to the same word, need to merge these in fixcorp functions (may have done this).
7. save/load trained models see issue #13.
8. stream documents from disk.
9. what variables *must* be finite in check_model, currently sigma and invsigma not required to be finite
10. should sigma for CTM be required to be positive definite or just positive semidefinite? test with 2 topics maybe. could set inv(model.sigma) to pinv(model.sigma)
11. get NaNs with LDA with 1 topic. Basically, gamma and alpha go to Inf with 1 topic because that increasing the likelihood (may have been fixed).
12. pull request to Distributions.jl for Dirichlet with length 1 parameter, entropy should always be zero, but it's not for maxfloat(1.0) (possibly related to PR).
13. findall for docs in corpus not working, need to understand how to do this correctly.
14. should possibly define abstract types, AbstractLDA = Union{LDA, fLDA, gpuLDA}, same with CTM and CTPF
15. need to deal with beta and fbeta in filtered models, predict function, etc.
16. need to deal with mu and lambda in CTM with floatmax, etc.
17. when you initialize beta as ones(K, V) / V, it doesn't train.
18. errors in showdocs, showtitles need to be standardized, show at the bad doc index, or check all indices at the beginning.
19. need to set up predict to return GPU models
20. change hardcoded K, V, M, etc. in check_model functions to string imputed values for model.
21. consider batch/stochastic optimization.
22. write CUDA GPU algorithms.
23. improve filtered models if possible.
24. write GPU algorithms for filtered models.
