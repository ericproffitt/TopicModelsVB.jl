1. smooth out fLDA and fCTM.
2. figure out way to preserve ordering of documents so showdocs() is comprehensible.
3. use feather or paraquet or some fast read-in binary file format for default corpora in order to speed up their load times.
4. deal with counts situation.
5. review update_alpha! for LDA models, determine if interior point Newton's method with sequence of decreasing barrier parameters nu is correctly designed.
6. save/load trained models see issue #13.
7. stream documents from disk.
8. what variables *must* be finite in check_model, currently sigma and invsigma not required to be finite.
9. CTM models appear to be overflow safe for large variational parameters (both additive_logistic and logsumexp are overflow safe, and since logzeta is updated first it prevents overflow in update_lambda! and update_vsq!), however large mu will result in overflow at update_lambda!, not sure about large sigma/invsigma.
10. pull request to Distributions.jl for Dirichlet with length 1 parameter, entropy should always be zero, but it's not for maxfloat(1.0).
11. findall for docs in corpus not working, need to understand how to do this correctly.
12. need to deal with mu and lambda in CTM with floatmax, etc.
13. when you initialize beta as ones(K, V) / V, it doesn't train.
14. need to set up predict to return GPU models.
15. change hardcoded K, V, M, etc. in check_model functions to string imputed values for model.
16. consider stochastic variational inference possibly with natural gradient (natural gradient likely not necessary since Newton's method is invariant under affine transformations, no gradient descent used in package, but is the natural gradient an affine transformation?).
17. write CUDA GPU algorithms.
18. write Metal GPU algorithms.
19. improve filtered models if possible.
20. write GPU algorithms for filtered models.
21. decide whether to replace check_doc, check_corp, check_model with checkdoc, checkcorp, and checkmodel.
22. make Document and Corpus parametric types for Int16 and Int32 to save memory.
23. further improve Document and Corpus error handling.
24. improve performance for gendoc, gencorp and predict.
25. determine if OpenCL kernels for phi and xi can be made more performant.
26. should consider switching order of update_lambda! and update_vsq!, and update_mu! and update_sigma! (basically update mean before variance) in coordinate ascent algorithm for CTM models.
27. for LDA models, large alpha causes overflow in update_Elogtheta! and problems with update_alpha!, update_Elogtheta! can be handeled by approx. digamma(∑x) ≈ log(∑x) for x >> 0, and then use overflow safe log(∑x), however update_alpha! still problematic.
28. test on other datasets.
29. could counts and ratings be allowed to be floating point?
30. topicdist for CTM is just a (very good) approx. to E_q[exp(x_i)/∑exp(x)].
31. CTM models appear to degenerate along a topic dimension for very large iterations, see issue #14, as a consequence of only having K-1 degress of freedom, potentially resolvable by applying suggested proj. normalization, or re-introducing Tikhonov regularization, make sure negative oscillatory behavior doesn't return.
32. change iter and tol to viter and vtol in predict functions.
33. need to copy/deepcopy train_model variables to model in predict functions, otherwise pass by reference risk between train_model and model.
34. need to put newlines at the end of each src file.

